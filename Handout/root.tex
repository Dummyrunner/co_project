%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document


% MYTHINGS
\usepackage{amssymb,amsmath, amsfonts,color}
\usepackage[ruled, vlined]{algorithm2e}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newcommand{\grad}{\nabla}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\Fx}{F(x)}
\newcommand{\jacF}{J F}
\newcommand{\xk}{x_k}
\newcommand{\Fxk}{F(\xk)}
\newcommand{\jacFxk}{\jacF (\xk)}
\newcommand{\xkplus}{x_{k+1}}
\newcommand{ \Lx}{L(x)}
\newcommand{\xtil}{\tilde{x}}
\newcommand{\fnull}{f_0}
\newcommand{\todo}{TODO!}
\newcommand{\xnull}{x_0}
\newcommand{\ind}[2]{{#1}_{\mathrm{#2}}}
\newcommand{\ifct}{\ind{I}{-}}
\newcommand{\ifcthat}{\ind{\hat{I}}{-}}
\newcommand{\xopt}{x^*}
\newcommand{\OptProblem}{
	\begin{aligned}
	& \underset{x}{\text{minimize}}
	& & f_0(x) \\
	& \text{subject to}
	& & f_i(x) \leq 0, \; i = 1, \ldots, m.\\
	& & &g_i(x) = 0, \; i = 1,\dots, p.
	\end{aligned}
	}
\newcommand{\OptProblemquad}{
	\begin{aligned}
		& \underset{x}{\text{minimize}}
		& & f_0(x) \todo auf quad anpassen!!\\
		& \text{subject to}
		& & f_i(x) \leq 0, \; i = 1, \ldots, m.\\
		& & &g_i(x) = 0, \; i = 1,\dots, p.
	\end{aligned}
}
% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
%
\title{\LARGE \bf Magic Gradient Descent*}


\author{Albert Author$^{1}$ and Bernard D. Researcher$^{2}$% <-this % stops a space
\thanks{*Project within the course Convex Optimization, University of Stuttgart, \today.}% <-this % stops a space
\thanks{$^{1}$Albert Author is a student of the Bachelor study program Mechatronics, University of Stuttgart,
        {\tt\small albert.author@papercept.net}}%
\thanks{$^{2}$Bernard D. Researcher is a student of the Master study program Engineering ... ., University of Stuttgart,
        {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Describe in a few sentences what the paper is about and why it is interesting 
to read it.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Some general introducing sentences about the topic, motivation and relevance of problem/algorithm.

In this paper we give an introduction to the results presented in paper(s) \cite{Bro-14}.



We present the problem statement (optimization problem)
the main results/algorithms, discuss the underlying ideas and illustrate the results 
by numerical simulations.

Notation. Define notation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PROBLEM STATEMENT AND BACKGROUND}

Provide a mathematical problem description. If necessary, some background material.

\begin{equation}
\OptProblem
\label{eq:OptProblem}
\end{equation}

-convex quadratic opt prblem with inequality constraints\\
-> how to handle ineq. constraints?
-technical relevance: optimal control, mpc


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MAIN RESULTS}
\subsection{Concept of Barrier Methods}
Convex optimization Problems with no inequality constraints can  be solved efficiently by using Newton's method. If inequality constraints are involved, Newton's method can not guarantee feasibiliy of a solution. It is hence desirable, to transform an inequality-constrained optimization problem into a only equality-constrained one. Therefore, we move the inequality constraints implicitley to the objective function. A simple and also precise way to do this, evaluate an  indicator function  
\begin{align}
	\ifct (x) :=
	\begin{cases}
		0 \quad &\text{for } u \neq 0\\
		\infty &\text{for } u > 0
	\end{cases}
\end{align}
on the values of the inequality constraints $ f_i, i=1,\dots,m $. Then, the optimization Problem has the shape
\begin{align}
	\begin{aligned}
	& \underset{x}{\text{minimize}}
	& & f_0(x) + \sum_{i=1}^{m} \ifct(f_i(x))\\
	& \text{subject to}
	& & g_i(x) = 0, \; i = 1,\dots, p.
	\end{aligned}
\end{align}
This problem is an equivalent to\ref{eq:OptProblem} and has no inequality constraints. However, it is clearly neither convex nor continuous (and hence not differentiable). Since we need these properties to solve the optimization problem computationally, we approximate the indicator function $ \ifct $ with the function
\begin{equation}
	\ifcthat (u) =
	\begin{cases}
	 \frac{1}{t}\log(-u) \quad \text{for } u < 0,\\
	 \infty \quad \text{for } u\geq 0,
	\end{cases}
\end{equation}
The parameter $ t>0 $ sets the approximation's accuracy. The higher $ t $ is, the better the indicator function is approximated.
%TODO evt plot von Ihat einfügen!


%TODO ápprox problem

Note, that $ \frac{1}{t}\log(-u) $ is convex, increasing in $ u $, and differentiable on the admissible set. Hence the entire 


\subsection{Newton's Method}
Newton's method is an iterative process to solve nonlinear equality systems
\begin{equation}
\Fx= 0
\end{equation}
for a differentiable map $ F: \Rn \longrightarrow \Rm $. The idea of this algorithm is as follows: At a given point $ \xk $, the zero of the linear approximation of $ F $ around $ \xk $  is computed. This point is chosen as the next iterate $ \xkplus $. In particular, a linear approximation of $ F $ in  $ \xk $ is defined as
\begin{equation}
	\Lx:= \Fxk + \jacFxk(x - \xk) \text{ for } x\in \Rn,
\end{equation}
where $ \jacFxk $ is the Jacobian of $ F $ at the point $ \xk $. If $ \jacFxk $ invertible, the point $ \xtil $ with $ L(\xtil)=0 $ is exactly the solution of  the linear equality $ \jacFxk x = -\Fx $.
%Though Newton's method is in general not guaranteed to converge,  
Technical conditions and proofs about convergence rates of Newton's method can be found in \cite{SO}.

%TODO Newton algo angeben??
%\begin{algorithm}
%	\SetAlgoLined
%	\KwResult{$ \xtil $, approximate solution of nonlinear system of equalities $ \Fx = 0$ }
%	initialization\; Function $ F: \Rn \longrightarrow \Rn $, initial point $ \xnull $\\
%	\While{While condition}{
%		instructions\;
%		\eIf{condition}{
%			instructions1\;
%			instructions2\;
%		}{
%			instructions3\;
%		}
%	}
%	\caption{Newton's Method}
%\end{algorithm}

For the purpose of optimizing a convex, twice differentiable objective function $ \fnull $ we want to find a zero of the gradient $ \grad \fnull $. Therefore we can apply the Newton Method to solve the non-linear equation $ \Fx := \grad \fnull (x) = 0 $. By convexity, satisfying $ \grad \fnull (\xopt) = 0$ is not only neccessary, but also sufficient for $ \xopt $ to be a global minimum of $ \fnull $.

Present main theorems/algorithm. Explain idea, explain algorithm, 
provide a convergence proof, discuss main properties (advantages and disadvantages)
%\begin{algorithm}
%	
%\end{algorithm}
Use algorithm environment in Latex to present algorithm (pseudo-code)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{EXAMPLES}

Show and discuss simulation examples etc....



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CONCLUSIONS}

Summarize the main points (with more details than in the preceding introduction).
The paper should not be between 4 and 8 pages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}

Add for example your Matlab code here. (Code should be nicely formated and documented).

Appendixes should appear before the acknowledgment.

\section*{ACKNOWLEDGMENT}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bibliographystyle{plain}
%\bibliography{c:/home/Lehre/VO/SS17/ND/Projects/Latex/mybib}

\begin{thebibliography}{99}
	  \bibitem{SO} Carsten Scherer {\em Vorlesungsskript Einführung in die Optimierung} 2019: Lehrstuhl für Mathematische Systemtheorie, Universität Stuttgart.
	  \bibitem{BV} Stephen Boyd, Lieven Vandenberghe {\em Convex Optimization} 2004: Cambridge University Press.
\end{thebibliography}

\end{document}
